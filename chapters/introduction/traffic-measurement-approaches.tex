%*****************************************************
\chapter{Traffic Measurement Approaches}\label{ch:traffic-measurement-approaches}
%*****************************************************

Researchers, service providers and security analysts have long been interested in network and user behavioral patterns of the traffic crossing the internet backbone. They want to use this information for the purpose of billing and mediation, bandwidth provisioning, detecting malicious attacks, network performance evaluation and overall improvement. Traffic measurement techniques that have been rapidly evolving in the last decade, have matured enough today to provide such an insight. In this chapter, we discuss some of these techniques and how they are being used to shape the future of the internet.

\section{Capturing Packets}\label{sec:capturing-packets}
In this technique, raw packets traversing a monitoring point are captured for traffic measurement. The measurements can be done either live or the packets can be saved in a trace file for offline analysis. The trace files can range from containing mere headers to entire packets depending on the level of detailed analysis required. 

\lstset{caption=\texttt{tcpdump}: Example, 
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black}, 	
			  label=lst:tcpdump-example, aboveskip=20pt, captionpos=b}
\begin{lstlisting}
$ tcpdump port 80 -w $FILE
$ tcpdump -r $FILE
\end{lstlisting}

\texttt{tcpdump} and \texttt{wireshark} are the most popular tools used for packet capture and analysis. \texttt{tcpdump} \cite{tcpdump-manpage} is a premier command-line utility that uses the \texttt{libpcap} \cite{pcap-manpage} library for packet capture. A simple example to capture and read \marginpar{tcpdump} the \ac{HTTP} traffic is described in listing $\ref{lst:tcpdump-example}$. The power of \texttt{tcpdump}  comes from the richness of its expressions, the ability to combine them using logical connectives and extract specific portions of a packet using filters. \texttt{wireshark} \cite{wireshark-manpage} is a \ac{GUI} application, aimed at both journeymen and packet analysis \marginpar{wireshark} ninjas. It supports a large number of protocols, has a straightforward layout, excellent documentation, and can run on all major operating systems. 

Several studies have made use of this approach to analyze the network traffic patterns. The authors in \cite{kxu:2005}, for instance use data mining methodologies to define clusters of behavior profiles by understanding the captured traffic of end hosts.  These clusters are then fed into classifiers to \marginpar{applicability} automatically identify anomalous behavior patterns that are of interest to network operators. Similar profiling of end-hosts traffic in performed in \cite{tkaragiannis:2005}, but at the transport layer. This effort focusses on making the approach tunable to strike out a balance between the amount of traffic classified and the accuracy achieved by analyzing the traffic at multiple levels of details. 

This approach benefits from the astounding level of detail it can provide. It allows deep packet inspection of the traces, thereby exposing even the application content being exchanged across the network. This calls for privacy concerns \marginpar{pros and cons} and can even bring in legal repercussions to make this technique unattractive for traffic analyzers today. In addition, the actual usage of this method comes at a higher price of its storage overhead and its inability to scale to larger setups.

\section{Capturing Flows}\label{sec:capturing-flows}
In this technique, packets traversing a monitoring point are not captured raw, instead they are aggregated together based on some common characteristics. The common characteristics are learnt by inspecting the packet headers as they cross the monitoring point. Flow-records resulting from such an aggregation are then exported to a collector for further analysis. 

NetFlow and \ac{IPFIX} are the two popular standards of \ac{IP} flow information export. NetFlow \cite{rfc3954} is a proprietary network protocol designed by Cisco Systems. It \marginpar{netflow} allow routers to generate and export flow records to a designated collector. The latest version, NetFlow v$9$ provides flexibility of user-tailored export templates, \ac{MPLS} and \ac{IP}v$6$ support and a larger set of flow keys. \ac{IPFIX} \cite{rfc5101} on the other hand is an open standard by \ac{IETF} deemed to be the logical successor of NetFlow v$9$ on which it is based. The novelty of \marginpar{ipfix} the standard lies in its ability to describe record formats at runtime using templates based on an extensible and well-defined information model. The data transfer mechanism is also simplistic and extensible by being unidirectional and transport protocol agnostic.  

The wide applicability of this approach is easily seen from the pervasive use of flow records for a vibrant set of network analysis applications. For instance, the authors in \cite{mkim:2004} use the flow characteristics in the traffic pattern to formalize a detection function that maps traffic patterns to different \ac{DoS} \marginpar{applicability} attacks, whereas in \cite{sdominik:2010}, the authors use the flow-record data to exploit timing characteristics of webmail clients to classify features that could identify webmail traffic from any other traffic running over HTTPS.

This is has been possible largely due to the hardware-assisted aggregation of the packets that has helped solve the storage overhead and \marginpar{pros and cons} scalability limitation of packet capturing techniques. Overcoming these limitations have eventually allowed researchers to perform network analysis over a larger dataset passing across high-speed links. However, with the ever-increasing bandwidth demands, the speed of the network links in the internet backbone is only slated to increase further, therefore the time is not too far when this issue might again scares us of its homecoming. 

\section{Remote Monitoring}\label{sec:remote-monitoring}
In this technique, dedicated monitoring probes are deployed on network segments to continuously collect vital statistics and perform network diagnostic operations. The probes are configured to proactively monitor the network and automatically check for error conditions to later log and notify them to the management station. 

The \ac{RMON} Framework \cite{rfc3577} for \ac{SNMP} \cite{rfc1157} defines a number of \ac{MIB} objects to be used by these monitoring probes. The \ac{RMON}-1 standard \cite{rfc2819} for instance, defines a \ac{MIB} \marginpar{rmon} module to collect statistics, capture and filter packet contents at the logical link layer. The architecture in this standard has been further extended with a feature upgrade by the \ac{RMON}-2 standard \cite{rfc4502} to support similar analysis up to the application layer.

The novelity of this technique lies in the ability to immediately communicate important information to the managing station using events and alarms. The constructs are extremely flexible in giving full control over what conditions will cause an alarm \marginpar{pros and cons} and subsequently what event will be generated. The event-driven nature of such a monitoring platform however still does not satisfy the requirements of traffic analysis applications since the data that is pushed out is highly aggregated and lacks enough details to be useful. 

\section{Remote Metering}\label{sec:remote-metering}
In this technique, meters are deployed at the network measurement points to capture flow data according to a predefined set of rules specified by the user. The model, as defined by the \ac{RTFM} working group \cite{rfc2722} has been designed to be protocol agnostic and restrictive in the amount of flow data that can transmitted across the network and stored to reduce the processing time of network analysis applications. 

The feature that sets this technique apart is the flexibility given to the user to specify their flow measurement requirements, thereby allowing them to filter out the traffic they do not care about. This calls for the users, to at the very outset analyze \marginpar{pros and cons} and freeze their requirements before they start off to capture the traffic. This is analogous to the flaws inherit in the waterfall model \cite{wroyce:1987} of software design, whereby one need to design the design before one designs it.