%\begin{figure}[h!]
  %\begin{center}
    %\includegraphics* [width=0.8\linewidth]{figures/benchmarks/filter-fv1-fv2}
    %\caption{Filter Stage: F$(v1)$ vs F$(v2)$}
    %\label{fig:fv1-fv2-filter}
  %\end{center}
%\end{figure}

The merger as required by the \ac{NFQL} specification suggests matching each
group record from one branch with every other record of each branch. This
leads to a complexity of $O(n^m)$ where $n$ is the number of filtered group
records and $m$ is the number of branches. The possible number of tries when
matching group records however can be reduced by sorting the group records on
the field offsets used for a match. The merger, is now smart enough to skip
over iterator permutations \marginpar{skipping iterator permutations on sorted
group records} when a state of a current field offset value may not allow any
further match beyond the index in the current branch. For such an optimization
to work, the filtered group records must be sorted in the order of field
offsets specified in the merger clause. Specifying the filtered group records
in any other order may lead to undefined behavior. This means, that the terms
in the group clause must be arranged in such a way so to align with the order
of terms in the merger clause.

The \ac{NFQL} specification bases the merger matches on the notion of matched
tuples. This means that a filtered group record can be written to a file
multiple times if it is part of multiple matched tuples. This situation is
very common and it worsens when different branches have similar filtered
groups records. Since, the function of the merger is to find a match of groups
records across branches based on a predefined \marginpar{matched collections
over matched tuples} condition, all the group records across branches that
satisfy the condition can be clubbed into one collection instead of separate
tuples. All the group records within a collection can then be written to the
file. This eliminates the inherent redundancy and significantly improves the
merger performance. The implementation of this approach incurs a
reimplementation of the ungrouper. The ungrouper, as a result now accepts a
collection of matched filtered group records as input. It then iterates over
each collection to unfold it groups and write their flow record members to
files.

The performance comparison of this approach against the one suggested by the
specification is tricky. The merger implementation of the original
specification is slow. It is so slow that it keeps churning the CPU for days
without results. The newer approach takes less than an hour. The performance
evaluation of this newer approach is performed against SiLK and is discussed
in more details in \marginpar{performance comparison and present issues} the
next chapter. The newer approach however is not currently pushed to
\texttt{master} git branch\footnote{The newer merger implementation is
available in the \texttt{fastmerger} git branch}. This is because the newer
merger implementation has not yet been tested against the regression suite.
Another reason is that, the newer merger implementation currently does not
adapt itself if the terms defined in the merger clause do not exist in the
grouper clause.




%The execution engine in F$(v1)$ used to read all the flow records of a
%supplied trace into memory before starting the processing pipeline.  Since,
%the filter stage uses the  supplied set of absolute rules to make a decision
%on whether or not to keep a flow record; it had to pass through the whole
%in-memory recordset \emph{again} to fill in the filter results. This technique
%involves multiple linear runs on the trace and therefore slows down when the
%ratio of number of filtered records to the total number of flow-records is
%high as shown in figure \ref{fig:fv1-fv2-filter}.

%The filter stage in F$(v2)$, therefore has now been merged with in-memory read
%of the trace. This means, a decision on whether or not to make room for a
%record in memory and eventually hold a pointer for it in filter results is
%done upfront as soon as the record is read from the trace. In addition, if a
%request to write the filter stage results to a flow-tools file has been made,
%the writes are also made as soon the filter stage decision is available,
%thereby  \marginpar{faster filter}  allowing reading-filtering-writing to
%happen in $O(n)$ time, where $n$ is the number of records in the trace. It is
%important to note that the filtered records are saved in common location from
%where they are referenced by each branch. This helps keep the memory costs at
%a minimum when multiple branches are involved. A publically available Netflow
%$v5$ trace
%\footnote{\url{http://traces.simpleweb.org/traces/netflow/netflow1/netflow000.tar.bz2}}
%was used to compare the performance of the new filter in F$(v2)$ with that
%from F$(v1)$ as shown in figure \ref{fig:fv1-fv2-filter}. The trace has around
%$20M$ flow-records. The queries executed are available in source repository
%\footnote{\url{http://github.com/vbajpai/mthesis-src/}} and have been omitted
%from here for brevity reasons. 
