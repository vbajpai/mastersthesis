%************************************************
\chapter{Flowy}\label{ch:flowy-design}
%************************************************

Flowy \cite{kkanev:thesis:2009}\cite{kkanev:2010} is the first prototype implementation of a stream-based flow record query language \cite{vmarinov:thesis:2009}\cite{vmarinov:2009}\cite{vmarinov:2008}. The query language allows to describe patterns in flow-records in a declarative and orthogonal fashion, making it easy to read and flexible enough to describe complex relationships among a given set of flows. 

\section{Python Framework}\label{sec:python-framework}
Flowy is written in Python. The framework is subdivided into two main modules: the validator module and the execution module. The validator module is used for syntax checking and interconnecting of all the stages of the processing pipeline and the execution module is used to perform actions at each stage of the runtime operation. 

\subsection{PyTables and PLY}\label{subsec:pytable-ply}
Flowy uses PyTables \cite{falted:2003} to store the flow-records. PyTables is built on top of the \ac{HDF} library and can exploit the hierarchical nature of the flow-records to efficiently handle large amounts of flow data. In addition, Flowy uses \ac{PLY} for generating a \ac{LALR} parser and providing extensive input validation, error reporting and validation on the execution module.

\subsection{Records}\label{subsec:records}
Flow-records are the principal unit of data exchange throughout Flowy's processing pipeline. The prototype implementation allows the \texttt{Record} class to be dynamically generated allowing future implementations to easily plug in support for \ac{IPFIX} or even newer versions of NetFlow \cite{rfc3954} exports. The \texttt{Record} class handles the reading of the flow-records and saving them in PyTables for future processing.

\subsection{Parsers and Statements}\label{subsec:parsers-statements}
The \texttt{parser} module holds definitions for the lexer and parser. The statements when parsed are implicitly converted into instances of classes defined in the \texttt{statement} module. The instances contain meta-information about the parsed statement such as the values, line numbers and sub-statements (if any).

\section{Processing Pipeline}\label{sec:processing-pipeline}
\begin{figure}[h!]	
\begin{center}
  \includegraphics* [width=1.0\linewidth]{figures/flowy-pipeline}	
  \caption{Flowy: Processing Pipeline \cite{vmarinov:2009}}
  \label{fig:flowy-pipeline}
\end{center}
\end{figure}
The pipeline consists of a number of independent processing elements that are connected to one another using UNIX-based pipes. Each element receives the content from the previous pipe, performs an operation and pushes it to the next element in the pipeline. Figure \ref{fig:flowy-pipeline} shows an overview of the processing pipeline. The flow record attributes used in this pipeline exactly correlate with the attributes defines in the \ac{IPFIX} Information Model specified in RFC 5102 \cite{rfc5102}. A complete description on the semantics of each element in the pipeline can be found in \cite{vmarinov:thesis:2009}

\subsection{Splitter}\label{subsec:splitter}
The splitter takes the flow-records data as input in the \texttt{flow-tools} compatible format. It is responsible to duplicate the input data out to several branches without any processing whatsoever. This allows each of the branches to have an identical copy of the flow data to process it independently.

\subsubsection{Splitter Implementation}\label{subsubsec:splitter-impl}
The \texttt{splitter} module handles the duplication of the \texttt{Record} instances to separate branches. Instead of duplicating each flow-record to every branch (as specified in the specification), the implementation follows a pragmatic approach by filtering the records beforehand against all the defined filter rules to determine which branches a flow-record might end up in and saves this information in a record-mask tuple of boolean flags. The \texttt{go(...)} method in the \texttt{Splitter} class then iterates over all the (record, record-mask) pairs to dispatch the records to corresponding branches marked by their masks using the \texttt{split(...)} method. The class uses branch names to branch objects mapping to achieve the dispatch.

\subsubsection{Splitter Validator}\label{subsubsec:splitter-validator}
The \texttt{splitter\_validator} module handles the splitter processing stage.
The \texttt{SplitterValidator} class within the module uses the \texttt{Parser} and \texttt{FilterValidator} instances passed to it to create a \texttt{Splitter} instance and its child \texttt{Branch} instances.

\subsection{Filter}\label{subsec:filter}
The filter performs \emph{absolute} filtering on the input flow-records data. The flow-records that pass the filtering criterion are forwarded to the grouper, the rest of the flow-records are dropped. The filter compares separate fields of a flow-record against either a constant value or a value on a different field of the \emph{same} flow-record. The filter cannot \emph{relatively} compare two different incoming flow-records

\subsubsection{Filter Implementation}\label{subsubsec:filter-impl}
The \texttt{filter} module handles the filtering stage of the pipeline. Since in the implementation the filtering stage occurs before the splitting stage, a single \texttt{Filter} class instance suffices for all the branches. Within the \texttt{filter} module, each filtering statement is converted into a \texttt{Rule} class instance, against which the flow-records are matched. The \texttt{Rule} instances are constructed using the (branch mask, logical operator, arguments) tuple. After matching the records against the rules, the record's branch mask is set and is then used by the splitter to dispatch the records to the filtered branches.

\subsubsection{Filter Validator}\label{subsubsec:filter-validator}
The \texttt{filter\_validator} module handles the filter processing stage. The \texttt{FilterValidator} class within the module uses the \texttt{Parser} instance passed to it to create a \texttt{Filter} instance once the check on semantical constraints have passed. The constraints involve checking whether records fields referenced in the filter definition exist, whether filters references in composite filter definitions exist and whether duplicate filter definitions are defined.

\subsection{Grouper}\label{subsec:grouper}
The grouper performs aggregation of the input flow-records data. It consists of a number of rule modules that correspond to a specific subgroup. A flow-record in order to be a part of the group should be a part of at-least one subgroup. A flow-record can be a part of multiple subgroups within a group. In addition a flow-record cannot be part of multiple groups. The grouping rules can be either absolute or relative. The newly formed groups which are passed on to the group filter can also contain meta-information about the flow-records contained within the group using the aggregate clause defined as part of the grouper query.

\subsubsection{Grouper Implementation}\label{subsubsec:grouper-impl}
The \texttt{grouper} module handles the grouping of flow-records data. The \texttt{Group} class instance contains group-record's field information required for absolute filtering. It also contains the first and last records of the group required for relative filtering of the group-records. The \texttt{AggrOp} class instance handles the aggregation of group-records. The allowed aggregation operations are defined in \texttt{aggr\_operators} module. Custom-defined aggregation operations are also supported using \texttt{--aggr-import} command line argument.

\subsubsection{Grouper Validator}\label{subsubsec:grouper-validator}
The \texttt{grouper\_validator} module handles the grouper processing stage.
The \texttt{GrouperValidator} class within the module uses the \texttt{Parser} and \texttt{SplitterValidator} instances passed to it to create a \texttt{Grouper} instance once the check on semantical constraints such as the presence of referenced names and non-duplicate names have passed. Three aggregation operations: \texttt{union(rec\_id), min(stime), max(etime)} are added by default to each \texttt{Grouper} instance.

\subsection{Group-Filter}\label{subsec:group-filter}
The group-filter performs \emph{absolute} filtering on the input group-records data. The group-records that pass the filtering criterion are forwarded to the merger, the rest of the group-records are dropped. The group-filter compares separate fields (or aggregated fields) of a flow-record against either a constant value or a value on a different field of the \emph{same} flow-record. The group-filter cannot \emph{relatively} compare two different incoming group-records

\subsubsection{Group-Filter Implementation}\label{subsubsec:group-filter-impl}
The \texttt{groupfilter} module handles the filtering of group-records. The \texttt{GroupFilter} class within the module iterates over the flow-records within the group and applies filtering rules across them. The filtering rules reuse the \texttt{Rule} class from the \texttt{filter} module. The flow-records are then added to the time index and stored in a pytables file for further processing. For groups that do \emph{not} have a group-filter defined for them, run through a \texttt{AcceptGroupFilter} class instance.

The \texttt{timeindex} module handles the mapping of the time intervals to the flow-records. The time index is used by the merger stage to learn about the records that satisfy the Allen relations. The \texttt{add(...)} method in the \texttt{TimeIndex} class is used to add new records to the time index. The \texttt{get\_interval\_records(...)} method on the other hand is used to retrieve records within a particular time interval.

\subsubsection{Group-Filter Validator}\label{subsubsec:group-filter-validator}
The \texttt{splitter\_validator} module handles the splitter processing stage.
The \texttt{SplitterValidator} class within the module uses the \texttt{Parser} and \texttt{FilterValidator} instances passed to it to create a \texttt{Splitter} instance and its child \texttt{Branch} instances.

\subsection{Merger}\label{subsec:merger}
The merger performs relative filtering on the N-tuples of groups formed from the N stream of groups passed on from the group-filter as input. The merger rule module consists of a number of a submodules, where the output of the merger is the set difference of the output of the first submodule with the union of the output of the rest of the submodules. The relative filtering on the groups are applied to express timing and concurrency constraints using Allen interval algebra \cite{fallen:1983}

\subsubsection{Merger Implementation}\label{subsubsec:merger-impl}
The \texttt{merger} module handles the merging of stream of groups passed as input. It is implemented as a nested branch loop organized in an alphabetical order where every branch is a separate \texttt{for-loop} over its records. During iteration, each branch loop executes the rules that matches the arguments defined in the group record tuple and subsequently passes them to the lower level for further processing. The \texttt{Merger} class represents the highest level branch loop and as such it must iterate over all of its records since it does not have any rules to impose restrictions on the possible records. The \texttt{MergerBranch} on the other hand represents an ordinary branch loop with rules.

\subsubsection{Merger Validator}\label{subsubsec:merger-validator}
The \texttt{splitter\_validator} module handles the splitter processing stage.
The \texttt{SplitterValidator} class within the module uses the \texttt{Parser} and \texttt{FilterValidator} instances passed to it to create a \texttt{Splitter} instance and its child \texttt{Branch} instances.

\subsection{Ungrouper}\label{subsec:ungrouper}
The ungrouper unwraps the tuples of group-records into individual flow-records, ordered by their timestamps. The duplicate flow-records appearing from several group-records are eliminated and are sent as output only once. 

\subsubsection{Ungrouper Implementation}\label{subsubsec:ungrouper-impl}
The \texttt{ungrouper} module handles the unwrapping of the group-records. The generation of flow-records can also be suppressed using \texttt{-no-records-ungroup} command line option. The \texttt{Ungrouper} class instance is initialized using a merger file and an explicit export order.

\subsubsection{Ungrouper Validator}\label{subsubsec:ungrouper-validator}
The \texttt{splitter\_validator} module handles the splitter processing stage.
The \texttt{SplitterValidator} class within the module uses the \texttt{Parser} and \texttt{FilterValidator} instances passed to it to create a \texttt{Splitter} instance and its child \texttt{Branch} instances.