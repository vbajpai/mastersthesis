%************************************************
\chapter{NFQL and Flowy}\label{ch:flowy-design}
%************************************************

Flowy \cite{kkanev:2010, kkanev:thesis:2009} is the first prototype
implementation of the \ac{NFQL} \cite{vmarinov:thesis:2009, vmarinov:2009,
vmarinov:2008}. The query language allows to describe patterns in flow-records
in a declarative and orthogonal fashion, making it easy to read and flexible
enough to describe complex relationships among a given set of flows.

\section{Flowy Python Framework}\label{sec:python-framework} 
Flowy is written in Python. The framework is subdivided into two main modules:
the validator module and the execution module. The validator module is used
for syntax checking and interconnecting of all the stages of the processing
pipeline and the execution module is used to perform actions at each stage of
the runtime operation.

Flowy uses PyTables \cite{falted:2003} to store the flow-records. PyTables is
built on top of the \ac{HDF} library and can exploit the hierarchical nature
of the flow-records to efficiently handle large amounts of flow data. The
\texttt{pytables} module provides methods to read/write to PyTables files. The
\texttt{FlowRecordsTable} class instance within \marginpar{pytables and ply}
the module exposes an iterator interface over the records stored in the HDF
file. The \texttt{GroupsExpander} class instance within the same module on the
other hand exposes an iterator interface over the group records and
facilitates ungrouping to flow records.  In addition, Flowy uses \ac{PLY} for
generating a \ac{LALR} parser and providing extensive input validation, error
reporting and validation on the execution modules.

Flow-records are the principal unit of data exchange throughout Flowy's
processing pipeline. The prototype implementation allows the \texttt{Record}
class (defined in the \texttt{record} module) to be dynamically generated
using \texttt{get\_record\_class(...)} allowing future implementations to
easily plug in support \marginpar{records} for \ac{IPFIX} or even newer
versions of NetFlow \cite{rfc3954} exports. The \texttt{FlowToolsReader} class
instance (defined in \texttt{ftreader} module) provides an iterator over the
records defined in \texttt{flow-tools} format. This can be plugged into the
\texttt{RecordReader} class instance (defined in \texttt{record} module) to
instantly get \texttt{Record} class instances.

The \texttt{parser} module holds definitions for the lexer and parser. The
statements when \marginpar{parsers and statements} parsed are implicitly
converted into instances of classes defined in the \texttt{statement} module.
The instances contain meta-information about the parsed statement such as the
values, line numbers and sub-statements (if any).



\section{NFQL Processing Pipeline}\label{sec:processing-pipeline}
The pipeline consists of a number of independent processing elements that are
connected to one another using UNIX-based pipes. Each element receives the
content from the previous pipe, performs an operation and pushes it to the
next element in the pipeline. Figure \ref{fig:flowy-pipeline} shows an
overview of the processing pipeline. The flow record attributes used in this
pipeline exactly correlate with the attributes defines in the \ac{IPFIX}
Information Model specified in RFC 5102 \cite{rfc5102}. A complete description
on the semantics of each element in the pipeline can be found in
\cite{vmarinov:thesis:2009}

\begin{figure}[h!]
\begin{center}
  \includegraphics* [width=1.0\linewidth]{figures/flowy-pipeline}
  \caption{Flowy: Processing Pipeline \cite{vmarinov:2009}}
  \label{fig:flowy-pipeline}
\end{center}
\end{figure}

The splitter takes the flow-records data as input in the \texttt{flow-tools}
compatible format. It is responsible to duplicate the input data out to
several \marginpar{splitter} branches without any processing whatsoever. This
allows each of the branches to have an identical copy of the flow data to
process it independently.

The \texttt{splitter} module handles the duplication of the \texttt{Record}
instances to separate branches. Instead of duplicating each flow-record to
every branch (as specified in the specification), the implementation follows a
pragmatic approach by filtering the records beforehand against all the defined
\marginpar{splitter implementation} filter rules to determine which branches a
flow-record might end up in and saves this information in a record-mask tuple
of boolean flags. The \texttt{go(...)} method in the \texttt{Splitter} class
then iterates over all the (record, record-mask) pairs to dispatch the records
to corresponding branches marked by their masks using the \texttt{split(...)}
method. The class uses branch names to branch objects mapping to achieve the
dispatch.

The \texttt{splitter\_validator} module handles the splitter processing stage.
The \marginpar{splitter validator} \texttt{SplitterValidator} class within the
module uses the \texttt{Parser} and \texttt{FilterValidator} instances passed
to it to create a \texttt{Splitter} instance and its child \texttt{Branch}
instances.

The filter performs \emph{absolute} filtering on the input flow-records data.
The flow-records that pass the filtering criterion are forwarded to the
grouper, \marginpar{filter} the rest of the flow-records are dropped. The
filter compares separate fields of a flow-record against either a constant
value or a value on a different field of the \emph{same} flow-record. The
filter cannot \emph{relatively} compare two different incoming flow-records

The \texttt{filter} module handles the filtering stage of the pipeline. Since
in the implementation the filtering stage occurs before the splitting stage, a
single \texttt{Filter} class instance suffices for all the branches. Within
the \texttt{filter} module, each filtering statement is converted into a
\texttt{Rule} \marginpar{filter implementation} class instance, against which
the flow-records are matched. The \texttt{Rule} instances are constructed
using the (branch mask, logical operator, arguments) tuple. After matching the
records against the rules, the record's branch mask is set and is then used by
the splitter to dispatch the records to the filtered branches.

The \texttt{filter\_validator} module handles the filter processing stage. The
\texttt{FilterValidator} class within the module uses the \texttt{Parser}
instance passed to it to create a \texttt{Filter} instance once the check on
semantical constraints \marginpar{filter validator} have passed. The
constraints involve checking whether records fields referenced in the filter
definition exist, whether filters references in composite filter definitions
exist and whether duplicate filter definitions are defined.

The grouper performs aggregation of the input flow-records data. It consists
of a number of rule modules that correspond to a specific subgroup. A
flow-record in order to be a part of the group should be a part of at-least
one subgroup. A flow-record can be a part of multiple subgroups within a
\marginpar{grouper} group. A flow-record cannot be part of multiple groups.
The grouping rules can be either absolute or relative. The newly formed groups
which are passed on to the group filter can also contain meta-information
about the flow-records contained within the group using the aggregate clause
defined as part of the grouper query.

The \texttt{grouper} module handles the grouping of flow-records data. The
\texttt{Group} class instance contains group-record's field information
required for absolute filtering. It also contains the first and last records
of the group required \marginpar{grouper implementation} for relative
filtering of the group-records. The \texttt{AggrOp} class instance handles the
aggregation of group-records. The allowed aggregation operations are defined
in \texttt{aggr\_operators} module.  Custom-defined aggregation operations are
also supported using \texttt{--aggr-import} command line argument.

The \texttt{grouper\_validator} module handles the grouper processing stage.
The \texttt{GrouperValidator} class within the module uses the \texttt{Parser}
and \texttt{SplitterValidator} instances passed to it to create a
\texttt{Grouper} instance once \marginpar{grouper validator} the check on
semantical constraints such as the presence of referenced names and
non-duplicate names have passed. Three aggregation operations:
\texttt{union(rec\_id), min(stime), max(etime)} are added by default to each
\texttt{Grouper} instance.

The group-filter performs \emph{absolute} filtering on the input group-records
data. The group-records that pass the filtering criterion are forwarded to the
merger, the rest of the group-records are dropped. The group-filter compares
\marginpar{group filter} separate fields (or aggregated fields) of a
flow-record against either a constant value or a value on a different field of
the \emph{same} flow-record. The group-filter cannot \emph{relatively} compare
two different incoming group-records

The \texttt{groupfilter} module handles the filtering of group-records. The
\texttt{GroupFilter} class within the module iterates over the flow-records
within the group and applies filtering rules across them. The filtering rules
reuse the \texttt{Rule} class from the \texttt{filter} module. The
flow-records are then added to the time index and stored in a pytables file
for further processing. For groups that do \emph{not} have a group-filter
defined for them, run \marginpar{group filter implementation} through a
\texttt{AcceptGroupFilter} class instance.  The \texttt{timeindex} module
handles the mapping of the time intervals to the flow-records. The time index
is used by the merger stage to learn about the records that satisfy the Allen
relations. The \texttt{add(...)} method in the \texttt{TimeIndex} class is
used to add new records to the time index. The
\texttt{get\_interval\_records(...)} method on the other hand is used to
retrieve records within a particular time interval.

The \texttt{groupfilter\_validator} module handles the group-filter processing
stage. The \texttt{GroupFilterValidator} class within the module uses the
\texttt{Parser} and \texttt{Grouper} instances passed to it to create an
instance of \texttt{GroupFilter}. \marginpar{groupfilter validator} The check
for the referenced fields is performed against the a ggregate clause defined
in grouper statements. The class instance uses the \texttt{AcceptGroupFilter}
instance in case a branch does \emph{not} have a group filter defined for it.

The merger performs relative filtering on the N-tuples of groups formed from
the N stream of groups passed on from the group-filter as input. The merger
rule module consists of a number of a submodules, where the output of the
\marginpar{merger} merger is the set difference of the output of the first submodule with the
union of the output of the rest of the submodules. The relative filtering on
the groups are applied to express timing and concurrency constraints using
Allen interval algebra \cite{fallen:1983}

The \texttt{merger} module handles the merging of stream of groups passed as
input. It is implemented as a nested branch loop organized in an alphabetical
order where every branch is a separate \texttt{for-loop} over its records.
During iteration, each branch loop executes the rules that matches the
arguments defined in the \marginpar{merger implementation} group record tuple
and subsequently passes them to the lower level for further processing. The
\texttt{Merger} class represents the highest level branch loop and as such it
must iterate over all of its records since it does not have any rules to
impose restrictions on the possible records. The \texttt{MergerBranch} on the
other hand represents an ordinary branch loop with rules.

The \texttt{merger\_validator} module handles the merger processing stage.
The \texttt{MergerValidator} class within the module uses the \texttt{Parser}
and \texttt{GroupFilterValidator} instances passed to it to create a
\texttt{Merger} instance once the check on referenced fields and branch names
has passed. In addition, \marginpar{merger validator} the validator also
ensures semantic checks on Allen algebra such as whether the Allen relation
arguments are correctly ordered, whether the Allen rules with the same set of
arguments are connected by an OR and whether each branch loop is reachable by
an Allen relation (or a chain of Allen relations) from the top level branch.

The ungrouper unwraps the tuples of group-records into individual
flow-records, ordered by their \marginpar{ungrouper} timestamps. The duplicate
flow-records appearing from several group-records are eliminated and are sent
as output only once.

The \texttt{ungrouper} module handles the unwrapping of the group-records. The
generation \marginpar{ungrouper implementation} of flow-records can also be
suppressed using the \texttt{-no-records-ungroup} command line option. The
\texttt{Ungrouper} class instance is initialized using a merger file and an
explicit export order.

The \texttt{ungrouper\_validator} module handles the ungrouper processing
stage.  The \marginpar{ungrouper validator} \texttt{UngrouperValidator} class
within the module uses the \texttt{Parser} and \texttt{MergerValidator}
instances passed to it to create a \texttt{Ungrouper} instance. This
processing stage does \emph{not} require any validation.


%\section{Future Outlook}\label{sec:flowy-future}
%\subsection{Reduced Copying}\label{subsec:reduced-copying}
%The \texttt{reset(...)} method of the \texttt{BranchMask} class performs a deepcopy on objects which significantly lowers performance. The invocation of this method can be inhibited by either removing the branch mask mechanism for simpler queries or removing it entirely. In addition avoiding usage of immutable containers (tuples) can also reduce internal copying during mutation.

%\subsection{Using PyTables in-kernel searches}\label{subsec:pytable-inkernel}
%PyTables can accelerate flow-records selection using a \texttt{where} iterator. The \texttt{where} clause is passed to the PyTables kernel which is written in C, therefore the selection can occur at C speed and only the filtered flow-records reach the Python space. This would require PyTables in-kernel search query support in the filtering rules and the \texttt{pytables} module would have to be extended to read from PyTables filtered flow-records.

%\subsection{Multithreaded Merger}\label{subsec:multithreaded merger}
%The merger stage in the processing pipeline is currently the most computation intensive operation and is unfortunately single-threaded. As suggested in \cite{kkanev:thesis:2009} it should be possible to handle the outermost branch loop using multiple threads in a non-blocking fashion to improve performance.
