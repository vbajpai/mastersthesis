%**************************************
\chapter{Flowy 2.0}\label{ch:flowy-2}
%**************************************

In an attempt to make the first prototype implementation of Flowy comparable with the contemporary flow-record analysis tools, the substitution of the performance hit sections of the Python code was thought out. Flowy 2.0 \cite{jschauer:thesis:2011} is the outcome of a complete rewrite of the core of the prototype implementation in C making it relatively faster in orders of magnitude. 

\section{Performance Issues and Improvements}\label{sec:performance-issues}
\begin{figure}[h!]
\begin{center}
  \includegraphics* [width=1.0\linewidth]{figures/flowy2-profiling}	
  \caption{Runtime Breakup of Individual Stages \cite{jschauer:thesis:2011}}
  \label{fig:flowy2-profiling}
\end{center}
\end{figure}
The runtime breakup of individual stages of the processing pipeline as shown in \ref{fig:flowy2-profiling} reveal that the grouper and merger incur a massive performance hit. A quick investigation hints towards usage of large deep nested loops in the merger with a worst-case $O(n^3)$ runtime complexity.

In addition, pushing the flow-records data from one stage of the pipeline to another involved deep copying of the whole flow data whereby a mere passing across of a reference across a pipeline in a branch would have sufficed. Similar behavior is visible when the grouper when passing group records saved the individual flow-records in a temporary location tagged with the groups and/or subgroups they belonged to.

The decision decision to use PyTables to read and write flow-records in \ac{HDF} format also added to the complexity. Since, the input flow-records were most of the time in either \texttt{flow-tools} or \texttt{nfdump} file-formats, each time they had to be converted into \ac{HDF} file formats prior to Flowy's execution which was unnecessary.

\subsection{Unmodified Parts}\label{subsec:unmodified-parts}
The flow-querier parser written in \ac{PLY} and the validators written for each stage of the processing pipeline that check for semantics correctness were left unmodified, since their execution time was invariant of the size of the input data and slightly varying on the query complexity in itself.

\subsection{Early Improvements}\label{subsec:early-improvements}
Thread affinity masks were set for each new thread created to delegate the thread to a separate processor core. \texttt{try/except} blocks were narrowed down to only code that needed to be exception handled. A test-suite was developed with few sample queries and input traces to validate Flowy's results for regression analysis. A \texttt{setup.py} script was written to facilitate installation of Flowy and its dependencies and \texttt{options.py} was replaced with \texttt{flowy.conf} configuration file with the standard human-readable key-value pairs. The command line option handling was switched from \texttt{optparse} to \texttt{argparse} module and a switch was added for easy profiling. The profiling output was modified as well to allow standard tab delimiters which can be easily parsed by other tools. The flow query was also  extended to allow file contents to be supplied using \texttt{stdin}. Variable names that are now part of Python identifiers were renamed. 

A C library was written to parse and read/write flow-records in \texttt{flow-tools} compatible format. The C library was connected to the Python prototype using Cython \cite{dseljebotn:2009}\cite{wilbers:2009}. This allowed the flow-records to be easily referenced by an identifier, thereby giving away the need to every time copy all the flow-records when moving ahead in the processing pipeline. Cython was used since it allowed to write C extensions in a Pythonic way by strong-typing variables, calling native C libraries and allowing usage of pointers and structs, thereby providing the best of both worlds \cite{sbehnel:2011}. 

\subsection{Data Format}\label{subsec:data-format}
A custom C library was written to directly read/write data in the \texttt{flow-tools} format to provide a drop-in replacement for PyTables and overcome the overhead of format conversions. The library sequentially reads the complete flow-records into memory to support random access required for relative filtering. Each flow-record is stored in a \texttt{char} array and the offsets to each field are stored in a separate \texttt{struct}. The array of such records are indexed allowing fast retrieval in $O(1)$ time. The C library is currently limited to support \emph{only} \texttt{flow-tools} formats; \texttt{nfdump} file formats are yet to be supported.

\subsection{Rewrite of Core Algorithms in C}\label{subsec:core-alg-c}
A design decision was made to rewrite the entire processing pipeline in C. However, currently the core cannot parse the flow-query file, therefore the execution is triggered by a tedious manual filling of the \texttt{structs} by the contents of the query. 

\lstset{caption=Filter Rule Struct \cite{jschauer:thesis:2011}, 
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black}, 	
			  label=lst:filterrule, belowskip=10pt}
\begin{lstlisting}
struct filter_rule {
	size_t field_offset;
	uint64_t value;
	uint64_t delta;
	bool (*func)(
		char *record,
		size_t field_offset,
		uint64_t value,
		uint64_t delta);
};
\end{lstlisting}

A filter stage \texttt{struct} is shown in listing \ref{lst:filterrule}. The field to be filtered is indicated using a \texttt{field\_offset} and \texttt{field\_length} in the \texttt{char} array of a records. The value to be compared against with is also supplied which can be either a static value or another field of a record. \texttt{func} is a function pointer to the operation that is to be carried out on a record whose record identifier is passed to it. The filter runs in $O(n)$ time as it needs to traverse through all the records of the \texttt{char} array.

\lstset{caption=Merger Rule Struct \cite{jschauer:thesis:2011}, 
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black}, 	
			  label=lst:mergerrrule, belowskip=10pt}
\begin{lstlisting}
struct merger_rule {
	size_t branch1;
	size_t field1;
	size_t branch2;
	size_t field2;
	uint64_t delta;
	bool (*func)(struct group *group1,
		size_t field1,
		struct group *group2,
		size_t field2,
		uint64_t delta);
};
\end{lstlisting}

Similarly, a merger stage \texttt{struct} is shown in listing \ref{lst:mergerrrule}. \texttt{branch\{1,2\}} are branch identifiers and \texttt{field\{1,2\}} are the aggregated field identifers in the order of aggregation. \texttt{func} is a function pointer pointing to the operation to be carried out. The merger runs in $O(n^k)$ time where $k$ is the number of branches. The \texttt{char} arrays in each branch are disjoint since a record cannot be part of more than one group.

The current core implementation also strictly adheres to the processing pipeline shown in figure \ref{fig:flowy-pipeline}. As such, it is not currently possible to skip stages. In addition it is not currently possible to have more than one merger or grouper in the flow-query or aggregate fields in the grouper module since \texttt{char} array storage is not possible. 

To overcome the limitations, the future outlook at this stage is to allow the Python prototype to parse and validate the flow query file which in turn would pass the contents to a Cython wrapper which on the fly will forward them to the core to properly fill in the \texttt{structs}. 

\section{Benchmarks}\label{sec:benchmarks}
\section{Future Outlook}\label{sec:flowy2-future}
	\subsection{System Integration}\label{subsec:system-integration}
	\subsection{Searching with Trees}\label{subsec:search-trees}
	\subsection{Specialized Functions in Inner Loops}\label{subsec:special-fns}
	\subsection{Efficient Multithreading}\label{subsec:multithreading}
	\subsection{Additional Functionality}\label{subsec:additional-functionality}