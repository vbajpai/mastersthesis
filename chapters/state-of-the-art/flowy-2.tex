%**************************************
\chapter{F $(v1)$}\label{ch:flowy-2}
%**************************************

In an attempt to develop a prototype implementation of \ac{NFQL} which will be
comparable with the contemporary flow-record analysis tools, the substitution
of the performance hit sections of the python code was thought out. Flowy
$2.0$ \cite{jschauer:thesis:2011} is the first attempt to try rewriting the
core of the prototype implementation in C.

\section{Flowy Performance Issues}\label{sec:performance-issues}

\begin{table}[h!]
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	no. of records & overall & filter & grouper & merger \\
	\hline
	\hline
	$103k$ & $1177s$ & $28s (2\%)$ & $240s (20\%)$ & $909s (77\%)$\\
	\hline
	$337k$ & $20875s$ & $110s (1\%)$ & $2777s (13\%)$ & $17988s (86\%)$\\
	\hline
	$656k$ & $70035s$ & $202s (0\%)$ & $8499s (12\%)$ & $61334s (87\%)$\\
	\hline
	$868k$ & $131578s$ & $274s (0\%)$ & $15913s (12\%)$ & $115391s (87\%)$\\
	\hline
	$1161k$ & $234714s$ & $1212s (1\%)$ & $25480s (11\%)$ & $208022s (88\%)$\\
	\hline
	\end{tabular}
\caption{Runtime Breakup of Individual Stages \cite{jschauer:thesis:2011}}
\label{tab:flowy2-profiling}
\end{table}

The runtime breakup of individual stages of the processing pipeline as shown
in table \ref{tab:flowy2-profiling} reveal that the grouper and merger incur a
massive performance hit. A quick \marginpar{deep nested loops} investigation
hints towards usage of large deep nested loops in the merger with a worst-case
$O(n^3)$ runtime complexity.

In addition, pushing the flow-records data from one stage of the pipeline to
another involved deep copying of the whole flow data whereby a mere passing
across of a \marginpar{deep copy of flow records} reference across a pipeline
in a branch would have sufficed. Similar behavior is visible when the grouper
when passing group records saved the individual flow-records in a temporary
location tagged with the groups and/or subgroups they belonged to.

The decision to use PyTables to read and write flow-records in
\ac{HDF} format also added to the complexity. Since, the input flow-records
were most of the time \marginpar{pytables and hdf} in either
\texttt{flow-tools} or \texttt{nfdump} file-formats, each time they had to be
converted into \ac{HDF} file formats prior to Flowy's execution which was
unnecessary.


\section{Preliminary Improvements}\label{sec:prelim-improvements}

The python parser written in \ac{PLY} and the validators written for
each stage of the processing pipeline that check for semantics correctness
were left unmodified, since their execution time was invariant of the size of
the input data and slightly varied on the query complexity.

Thread affinity masks were set for each new thread created to delegate the
thread to a separate processor core. \texttt{try/except} blocks were narrowed
down to only code that needed to be exception handled. A test-suite was
developed with few sample queries and input traces to validate Flowy's results
for regression analysis. A \texttt{setup.py} script was written to facilitate
installation of \marginpar{affinity masks, easier installation and
configuration, better profiling and testing, extended command line switches}
Flowy and its dependencies and \texttt{options.py} was replaced with
\texttt{flowy.conf} configuration file with the standard human-readable
key-value pairs. The command line option handling was switched from
\texttt{optparse} to \texttt{argparse} module and a switch was added for easy
profiling. The profiling output was modified as well to allow standard tab
delimiters which can be easily parsed by other tools. The flow query was also
extended to allow file contents to be supplied using \texttt{stdin}. Variable
names that are now part of Python identifiers were renamed.

A custom C library was written to directly read/write data in the
\texttt{flow-tools} format to provide a drop-in replacement for PyTables and
overcome the overhead of format conversions. The library sequentially reads
the complete flow-records into memory to support random access required for
relative filtering. Each flow-record is stored in a \texttt{char} array and
the offsets to each field are stored in a separate \texttt{struct}.
\marginpar{a custom c library to replace pytables, cython to connect the
library to python} The array of such records are indexed allowing fast
retrieval in $O(1)$ time.  The C library was connected to the Python prototype
using Cython \cite{dseljebotn:2009}\cite{wilbers:2009}.  This allowed the
flow-records to be easily referenced by an identifier, thereby giving away the
need to every time copy all the flow-records when moving ahead in the
processing pipeline.  Cython was used since it allowed to write C extensions
in a Pythonic way by strong-typing variables, calling native C libraries and
allowing usage of pointers and structs, thereby providing the best of both
worlds \cite{sbehnel:2011}.


%The current core implementation also strictly adheres to the processing
%pipeline shown in figure \ref{fig:flowy-pipeline}. As such, it is not
%currently possible to skip \marginpar{core limitations} stages. In addition it
%is not currently possible to have more than one merger or grouper in the
%flow-query or aggregate fields in the grouper module since \texttt{char} array
%storage is not possible.



\section{Rule Interfaces}\label{sec:rule-interfaces}


A design decision was made to attempt to rewrite the entire processing
pipeline in C.  However, currently the core can only run absolute filters and
cannot parse the flowquery file. Therefore the execution is triggered by a
tedious manual filling of the \texttt{struct} by the contents of the query.  A
filter \marginpar{filter stage struct} stage \texttt{struct} is shown in
listing \ref{lst:filterrule}. The field to be filtered is indicated using a
\texttt{field\_offset} and \texttt{field\_length} in the \texttt{char} array
of a records. The value to be compared against with is also supplied which can
be either a static value or another field of a record.  \texttt{func} is a
function pointer to the operation that is to be carried out on a record whose
record identifier is passed to it. The filter runs in $O(n)$ time as it needs
to traverse through all the records of the \texttt{char} array.

\lstset{caption=Filter Rule Struct \cite{jschauer:thesis:2011},
				basicstyle=\tiny\ttfamily, tabsize=2, language=C, numbers=left,
        stepnumber=1, numberstyle=\ttfamily\color{gray},
        keywordstyle=\color{blue}, frame=shadowbox, rulesepcolor=\color{black},
			  label=lst:filterrule, aboveskip=20pt, captionpos=b}
\begin{lstlisting}
struct filter_rule {
	size_t field_offset;
	uint64_t value;
	uint64_t delta;
	bool (*func)(
		char *record,
		size_t field_offset,
		uint64_t value,
		uint64_t delta);
};
\end{lstlisting}


\section{Efficient Rule Processing}\label{sec:efficient-rule-processing}

The comparison operations, previously were required to make costly checks on
the length of the field type passed to them, to be able to make appropriate
casts. Such checks are now no longer needed. F $(v1)$ now allows filtering of
records via two methods: using specialized comparison functions or using one
main fall through \texttt{switch} statement. The implementation defaults to
using specialized comparison functions to encourage modularity in source code.

\lstset{caption=Auto Generated Comparison Functions \cite{jschauer:thesis:2011},
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black},
			  label=lst:autocompfunc, aboveskip=20pt, captionpos=b}
\begin{lstlisting}
bool filter_eq_uint8_t(...);
bool filter_eq_uint16_t(...);
...
\end{lstlisting}

In the default method, there is a comparison function defined for every
possible field length $(33)$ and comparison operations $(19)$. These functions
are generated using a Python script
\footnote{\texttt{scripts/generate-functions.py}} and are declared/defined in
\texttt{auto\_comps.\{h,c\}} as shown in listing $\ref{lst:autocompfunc}$. The
rule definitions are now able to make calls \marginpar{using function pointers
and switch cases} using a function name derived from the combination of field
length, delta type and operation. This subverts the need to define complex
branching statements and reduces complexity. In the other method, the logic is
executed by comparing the field length and the operation by falling through a
huge switch statement.  Such a huge switch statement is again generated using
the same Python script and is defined in \texttt{auto\_switch.c} as shown in
listing $\ref{lst:autoswitch}$.


\lstset{caption=Auto Generated Switch Statement \cite{jschauer:thesis:2011},
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black},
			  label=lst:autoswitch, aboveskip=20pt, captionpos=b}
\begin{lstlisting}
switch (group_modules[k].op) {
  case RULE_EQ | RULE_S1_8 | RULE_S2_8 | RULE_ABS:
  case RULE_EQ | RULE_S1_8 | RULE_S2_8 | RULE_REL:
  ...
\end{lstlisting}





\section{Faster Grouping Approach}\label{sec:faster-grouper}
A typical grouper module is shown listing \ref{lst:fv2-grouper-module}. In
order to be able to make comparisons on field offsets, the grouper initially
creates a copy of the pointers in the filtered recordset. A n\"aive approach
is to linearly walk through \marginpar{possible grouping approaches} all the
pointers against each pointer in the copy leading to a complexity of $O(n^2)$.
A smarter approach is to put the copy in a hash table and then try to map each
pointer while walking down the recordset once, leading to a complexity of
$O(n)$. The hash table approach, although will work on this specific example,
will fail badly on other relative comparisons.

\lstset{caption=A Grouper Module,
				tabsize=2, language=bash, numbers=left,stepnumber=1,
        basicstyle=\tiny\ttfamily, numberstyle=\ttfamily\color{gray},
        keywordstyle=\color{blue}, frame=shadowbox,
        rulesepcolor=\color{black}, label=lst:fv2-grouper-module,
        aboveskip=20pt, captionpos=b}
\begin{lstlisting}
grouper g1 {
  srcIP = srcIP
  dstIP = dstIP
}
\end{lstlisting}

It is clear that a middle ground compromise was needed. As a result, using a
binary search after a quick sort on the filtered recordset was thought out. To
achieve this, the array of pointers to the copy are sorted according to the
offset on the right side of the comparsion of the first grouping rule.
\marginpar{using quick sort and binary search} Such a sorted array of pointers
is then traversed linearly to find unique values. This helps the grouper
perform binary searches to find records that will eventually group together.
The preprocessing step takes $O(n*lg(n)) + O(n)$ in the worst case, with a
$O(n*lg(k))$ for binary search on the entire recordset. The implementation of
this idea is currently broken and segfaults at multiple stages. In essence,
this approach is not currently used in practise and quite some effort is
needed to bring it to life.


\section{Benchmarks}\label{sec:benchmarks}

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|}
		\hline
		Number of records & Flowy & F $(v1)$ \\
		\hline
		\hline
		$103k$ & $1177s$ & $0.3s$\\
		\hline
		$337k$ & $20875s$ & $3.4s$\\
		\hline
		$656k$ & $70035s$ & $13s$\\
		\hline
		$868k$ & $131578s$ & $23s$\\
		\hline
		$1161k$ & $234714s$ & $86s$\\
		\hline
		\end{tabular}
	\end{center}
\caption{Flowy vs $F (v1)$ \cite{jschauer:thesis:2011}}
\label{tab:flowy2-benchmarks}
\end{table}

The benchmarks in table \ref{tab:flowy2-benchmarks} show the relative
comparison of Flowy \cite{kkanev:thesis:2009} with F $(v1)$
\cite{jschauer:thesis:2011}. As the author appropriately points out, an
accurate comparison of Flowy with F $v1$ cannot be done accurately, since, the
the C execution engine \marginpar{vs flowy} lacks many feature-set and
pipeline stages. However, running a common flow query that can be run on both
the implementations was used and it was not surprising that F $(v1)$ was in
orders of magnitude faster in comparison.

\lstset{caption=F$(v1)$ vs {\texttt{flow-tools, nfdump}}
\cite{jschauer:thesis:2011},
				tabsize=2, language=C, numbers=left,stepnumber=1,
				numberstyle=\ttfamily\color{gray}, keywordstyle=\color{blue},
				frame=shadowbox, rulesepcolor=\color{black},
			  label=lst:f-benchmark-queries, aboveskip=20pt, captionpos=b}
\begin{lstlisting}
src port 80
src port 80 or dst port 25
src port 443 or (src port 80 and dst port 25)
\end{lstlisting}

In order to evaluate how well F now performs with these added improvements,
the authors decided to compare it with the state-of-the-art flow-processing
tools: \marginpar{vs flow-tools and nfdump} \texttt{flow-tools}
\cite{sromig:2000} and \texttt{nfdump} \cite{phaag:2006}. A set of $3$ queries
involving only absolute filters was defined as shown in listing
$\ref{lst:f-benchmark-queries}$ and evaluated on a set of $500K-10M$
flow-records. It turned out that F $(v1)$ performed as well if not better than
the other flow tools.
